Original Article (8302 words):
Towards Goal-oriented Prompt Engineering
for Large Language Models: A Survey
Haochen Li
Jonathan Leung
Zhiqi Shen
Nanyang Technological University, Singapore
{haochen003, jonathan008, zqshen}@ntu.edu.sg
Abstract
Large Language Models (LLMs) have shown
prominent performance in various downstream
tasks and prompt engineering plays a pivotal
role in optimizing LLMs’ performance. This
paper, not only as an overview of current
prompt engineering methods, but also aims to
highlight the limitation of designing prompts
based on an anthropomorphic assumption that
expects LLMs to think like humans. From our
review of 50 representative studies, we demon-
strate that a goal-oriented prompt formulation,
which guides LLMs to follow established hu-
man logical thinking, significantly improves
the performance of LLMs. Furthermore, We
introduce a novel taxonomy that categorizes
goal-oriented prompting methods into five in-
terconnected stages and we demonstrate the
broad applicability of our framework. With
four future directions proposed, we hope to
further emphasize the power and potential of
goal-oriented prompt engineering in all fields.
1
Introduction
Large Language Models (LLMs) have garnered
significant interest with their ability to assimilate
vast amounts of information from large corpora
of data. They have demonstrated proficiency in
open-domain dialogue (Deng et al., 2023; Yu et al.,
2023a), reasoning (Wei et al., 2022; Yao et al.,
2023b; Besta et al., 2023), planning (Song et al.,
2023; Liu et al., 2023a), and many more tasks.
The predominant approach to leveraging LLMs
for downstream tasks involves crafting tailored text
prompts to tap into LLMs’ potential (Liu et al.,
2023d). However, the effectiveness of LLMs is
contingent upon the prompting strategy employed,
leading to performance disparities (Zhao et al.,
2021). Such a phenomenon has given rise to the
field of prompt engineering, which investigates the
optimal formulation of prompts for specific tasks.
To prompt LLMs appropriately, it is important
to first understand the deviation in the human way
of thinking and LLMs’ way of processing informa-
tion. Humans are taught by experience to approach
complex objectives by breaking down the main
goal into more manageable sub-goals, a strategy
supported by goal-setting theory (Austin and Van-
couver, 1996). In interacting with LLMs, which of-
ten display human-like conversational abilities, we
might be tempted to attribute human-like thought
processes to them, expecting them to know from
the start that they should decompose complex prob-
lems into simpler tasks. This anthropomorphic as-
sumption does not hold for LLMs and can lead
to unsatisfactory outcomes (Abercrombie et al.,
2023). On the other hand, as shown in our lit-
erature review of 50 representative studies, if we
design the prompts from a goal-oriented perspec-
tive that guides LLMs to mimic human thinking,
LLMs’ performance improves significantly. In the
meantime, aligning LLM’s behavior with human
logic allows more human-interpretable answers
from LLMs and results in more effective and reli-
able human-computer interaction.
While several surveys have explored LLM
prompting, none have examined it from a goal-
oriented perspective or explicitly summarized the
task-agnostic internal steps involved in designing
and refining prompting procedures1. In this pa-
per, we propose the first goal-oriented taxonomy of
prompting methods, which classifies goal-oriented
prompting methods into goal decomposition, action
selection, action execution, sub-goal result evalu-
ation, and sub-goal selection, as shown in Fig. 1.
We further classify the surveyed methods by their
targeted task and analyze their respective perfor-
mance to provide readers with a better estimate of
the current progress of goal-oriented prompt engi-
neering. Finally, we offer four promising future
directions, including hierarchical decomposition,
efficiency problems, the synergy of stages, and
1We discuss related surveys in Appendix A.
arXiv:2401.14043v3  [cs.CL]  17 Sep 2024
How many balls does he buy?
How many balls does he have at beginning?
How many balls does he have if he buys one can?
How many balls does he buy? The answer is 6.
How many balls does he have at beginning? 
The answer is 5.
How many balls does he have if he buys one 
can? The answer is 8.
Roger has 5 tennis balls. He buys 2 more cans of 
tennis balls. Each can has 3 tennis balls. How 
many tennis balls does he have now?
The final 
answer is 11.
Goal
Sub-goal
Action Selection
Action Execution
Result Evaluation
Valuable Sub-goal Selection
After checking, the calculation 
result is correct.
[Use a calculator] The answer is 6.
2 cans of 3 tennis balls each. We should 
calculate 2×3 to know how many new
balls he buys.
Figure 1: An overview of the goal-oriented framework for prompting LLMs taking solving a math word problem
as an example. (1) Decomposing goal into sub-goal sequences. (2) Action selection for attaining sub-goals. (3)
Executing actions to get sub-goal results. (4) Evaluating sub-goal results. (5) Further selection of valuable
sub-goals. Note that stages (2)(3)(4) are taken for all the decomposed sub-goals.
broader applications.
2
Preliminary
In this section, we provide a definition of goal-
oriented prompt engineering. Before define goal-
oriented prompting, we can first define standard
prompting, the process of prompting LLM PrLLM
for a prediction on a given goal as:
Pr(P|G) =
|P|
Y
i=1
PrLLM(pi|G, p<i)
(1)
By introducing additional variables sub-goal g,
action a, and sub-goal result r, we formally define
goal-oriented prompt engineering as the application
of a multi-stage strategy to guide LLMs in problem-
solving by adhering to established human logic, as
shown in Fig. 1.
In
goal-oriented
prompt
engineering,
the
overall goal G is first decomposed into sub-
goals {g1, g2, ..., gn} (Fig.1(1)),
then actions
{a1, a2, ..., an} are selected to achieve correspond-
ing sub-goals (Fig.1(2)). The final prediction is
based on valuable sub-goal results {r1, r2, ..., rn}
(Fig.1(5)). The process could be described as:
Pr(P|G) =
n
X
i=1
Pr(P|ri, ai, gi, G)
(2)
Pr(ri|ai, gi, G)Pr(ai|gi, G)Pr(gi|G)
If LLMs are applied for the implementation of
a stage, we could follow Eq. 1 to get the out-
put. We additionally introduce the action execution
(Fig.1(3)) and sub-goal evaluation stages (Fig.1(4))
to ensure the correctness of sub-goal results.
3
Taxonomy of Methods
In this section, we go into detail about the five steps
described in our goal-oriented framework in Fig. 1.
3.1
Decomposing Goal into Sub-goal
Sequences
Decomposing a high-level goal into sub-goals is
particularly useful for complex problems where a
straightforward answer isn’t sufficient. In this sec-
tion, we introduce the methods to decompose goals
and improve the performance of decomposition.
Iterative decomposition.
Iterative decomposi-
tion generates a sub-goal, gets sub-goal results,
and repeats this process with the knowledge of the
previous sub-goal and sub-goal results. Chain-of-
thought prompting (CoT) (Wei et al., 2022) can
be considered the first work that follows iterative
decomposition. With either “Let’s think step by
step.” (Kojima et al., 2022) in the prompt, or in-
termediate reasoning steps in in-context examples,
LLMs can imitate the decomposition process and
solve the problem step by step. Here, the intermedi-
ate reasoning steps can be considered as sub-goals,
and they are sequentially connected to form a sub-
goal sequence. CoT implicitly follows iterative
decomposition as LLMs generate tokens in an au-
toregressive way, with the next sub-goal decided
based on previous content.
There are also works that explicitly ask LLMs to
follow the iterative decomposition. DecomP (Khot
et al., 2022), Successive Prompting (Dua et al.,
2022), and Recursion-of-Thought (RoT) (Lee and
Kim, 2023) are three works that repeatedly prompt
LLMs to ask follow-up sub-questions for back-
ground knowledge. Self-ask (Press et al., 2022a)
improves efficiency by designing a template for
follow-up questions in advance. With that tem-
plate, LLMs can generate all essential questions
and their answers by prompting once. Besides, in
this paper, the authors empirically show that LLMs
are often wrong when asking them to answer a com-
plex question even if they know the true answer of
all needed sub-questions. This finding indicates the
significance of decomposing complex goals into
simple sub-goals for LLMs.
Plan-then-execute decomposition.
In contrast
to iterative decomposition, plan-then-execute de-
composition methods generate the sub-goal se-
quence all at once, which means that the latter
sub-goal will not be affected by the result of the
former ones. For example, Least-to-most prompt-
ing (Zhou et al., 2022) only prompts LLMs two
times, one for generating a plan to decompose
the goal into a sub-goal sequence, and the other
one for executing the plan. Plan-and-solve prompt-
ing (Wang et al., 2023d) further improves the effi-
ciency of Least-to-most prompting by merging the
generation of the plan and execution of the plan
into one output. DEPS (Wang et al., 2023f) and
GITM (Zhu et al., 2023) are decomposition meth-
ods designed for Minecraft, an open-world game
in which an agent can craft different items and
tools. In this game, obtaining base materials that
are needed to craft a target item can be viewed as
sub-goals. While DEPS generates a plan to obtain
required objects in sequence solely based on LLMs,
GITM leverages a pre-defined sub-goal tree to help
LLMs locate prerequisites more precisely.
External decomposition.
The above two cate-
gories rely on LLM’s knowledge to decompose
the goal into sub-goals. However, due to the hal-
lucination problem (Ji et al., 2023), LLMs some-
times generate seemingly plausible sub-goals but
not grounded in reality. To ensure the accuracy
of decomposition, LLM+P (Liu et al., 2023a) and
SayPlan (Rana et al., 2023) take advantage of clas-
sical planners. They use LLMs to translate goals
written in natural language to planning domain def-
inition language so that classical planners can deal
with them. The output of planners will then be
translated back into natural language by LLMs for
execution.
3.2
Action Selection for Attaining Sub-goals
With sub-goals defined, another important step is
action selection, choosing effective and valid ac-
tions to reach desired outcomes. CoT is a naive
way for action selection where it is completely
decided by LLMs themselves. However, due to
the hallucination problem, actions from LLMs are
often invalid. In this section, we classify the ad-
vanced action selection methods into two classes,
constrain-then-select and select-then-mapping.
Constrain-then-select.
Constrain-then-select
method predefines an action space and then has
LLMs select the actual action among the space.
As in MWP (Zhang et al., 2023), to deal with
math word problems, the authors first employ
an operation prediction module to predict the
needed calculation operation (e.g. multiplication,
summation). LLMs are then used to select the
appropriate operands and complete the calculation.
PEARL (Sun et al., 2023) utilizes constrained
action space like “Finding the definition of
A”, “Compare A and B”, and “Summarize A”
for question answering over long documents.
DecomP (Khot et al., 2022) selects actions from
a set like “split” and “merge” for k-th letter
concatenation, a symbolic reasoning task. In robot
planning, given a task instruction, SayPlan (Rana
et al., 2023) applies semantic search to first
identify a task-relevant subgraph from the whole
3D scene graph, which makes it easier for LLMs
to plan based on the subgraph.
In dialogue
systems, RLP (Fischer, 2023), SAFARI (Wang
et al., 2023b), and Cue-CoT (Wang et al., 2023c)
set the mental states of the LLM and the user,
respectively, such as beliefs or desires, to guide
action selection.
Such a setting improves the
context richness, coherence, and interactivity
of LLM-based conversational systems. Similar
to PEARL, ProCoT (Deng et al., 2023) forms
pre-defined action sets covering query clarification,
topic transition, and negotiation strategy for
dialogue systems.
Select-then-mapping.
Different from constrain-
then-select, select-then-mapping first uses LLMs to
generate actions solely based on their knowledge
and then maps the generated action to the most
similar one in the valid action space. Zero-shot
planners (Huang et al., 2022b), SALP (Gramopad-
hye and Szafir, 2023), and Re-Prompting (Raman
et al., 2022) aim to solve agent planning prob-
lems in an interactive environment. In such vir-
tual environments, there are only a few admissi-
ble actions that can be applied. However, the ac-
tions produced naively by LLMs often cannot be
mapped exactly to those executable actions. To
remedy this, researchers employ a text-embedding
language model to translate LLM-generated ac-
tions into the most similar admissible actions by
calculating cosine similarity.
3.3
Executing Actions to Get Sub-goal Results
In CoT, LLMs rely on their knowledge for action
execution to get the sub-goal result. Limited by
hallucination, the results are sometimes wrong. For
example, even if LLMs correctly select multipli-
cation as the action to solve a math problem, the
calculation result can be wrong. As (Khot et al.,
2022) claimed, some tasks may not be feasible to
solve using only LLMs. With such consideration,
some works leverage external tools to guarantee a
precise sub-goal result.
Some studies specifically study one task (Khot
et al., 2022; Chen et al., 2022; Wang et al., 2023e).
To answer open-domain questions, DecomP (Khot
et al., 2022) applies an ElasticSearch-based re-
trieval system to retrieve knowledge from cer-
tain knowledge bases like Wikipedia. Program-
of-Thoughts (PoT) (Chen et al., 2022), Faithful
CoT (Lyu et al., 2023), PAL (Gao et al., 2023),
MathPrompter (Imani et al., 2023), LINC (Olaus-
son et al., 2023), and Logic-LM (Pan et al., 2023)
translate reasoning processes into executable codes
and then run them in an interpreter. In recom-
mender systems, Recmind (Wang et al., 2023e)
and InteRecAgent (Huang et al., 2023b) use SQL
to search for the history of a user in the database
while DOKE (Yao et al., 2023a) achieves it through
knowledge graphs.
There are additional studies that aim for multi-
tasking. Toolformer (Schick et al., 2023) specifi-
cally studied which APIs to call and when to call
them when using external tools. In their paper,
several tools are adopted, including a calculator, a
calendar, a BM25-based Wikipedia search engine,
and a fine-tuned language model (LM). The au-
thors pointed out that small LMs finetuned for spe-
cific tasks can outperform some LLMs on certain
task. Following the idea of Toolformer, Hugging-
GPT (Shen et al., 2023) further extends the avail-
able LMs to the scale of all LMs on open-sourced
machine learning communities like HuggingFace.
Specifically, in their paper, they evaluate Hugging-
GPT on nine types of NLP tasks, nine types of CV
tasks, four types of audio tasks, and two types of
video tasks. With the development of open-sourced
communities, this work can potentially achieve all
Feedback Source
Method
LLM
Self-refine (Madaan et al., 2023), Self-
Check (Miao et al., 2023), Reflex-
ion (Shinn et al., 2023), REFINER (Paul
et al., 2024)
Environment
SayPlan (Rana et al., 2023),
Re-
prompt (Raman et al., 2022),
In-
ner Monologue (Huang et al., 2022c),
GITM (Zhu et al., 2023), DEPS (Wang
et al., 2023f)
VLM
DEPS
(Wang et al., 2023f), LLM-
Planner (Song et al., 2023), Inner Mono-
logue (Huang et al., 2022c)
Interpreter
Self-debug
(Chen
et
al.,
2023),
INTERVENOR
(Wang
et
al.,
2023a), CRITIC (Gou et al., 2023),
MAF (Nathani et al., 2023)
Heuristic Rule
Reflexion (Shinn et al., 2023)
Human
Inner Monologue (Huang et al., 2022c),
DEPS (Wang et al., 2023f)
Search Engine
CRITIC (Gou et al., 2023), Verify-and-
Edit (Zhao et al., 2023)
Table 1: Feedback source used by existing works.
the sub-goals that can be described in language.
3.4
Evaluating Sub-goal Results
In CoT, the results generated by LLMs are assumed
to be correct hence there is no evaluation process
for the sub-goal results. The negative impact of
hallucination becomes more severe for sub-goal
results because errors may propagate along the
pipeline and result in huge deviations from the cor-
rect answer. Thus, it is necessary to get feedback
for sub-goal results at each step and correct them
in time through resampling. The types of feedback
sources are summarized in Table 1. Note that the
methods we review in this section can be applied
to evaluate the overall goal achievement as well.
Feedback from LLMs.
Self-refine (Madaan
et al., 2023; Huang et al., 2022a) is the first work
addressing the evaluation of sub-goal results. After
getting the output of LLMs, Self-refine feeds the
output combined with task-specific prompts to the
same LLM to get feedback for the output. Then,
the feedback and initial output are fed together to
LLMs for output refinement. The refinement pro-
cess iterates with all of the past feedback and refine-
ment appended to the prompt. SelfCheck (Miao
et al., 2023) decomposes the Self-refine evalua-
tion into finer-grained processes. First, actions and
sub-goal results are put to LLMs to summarize
(a) CoT
(b) Self-consistency
(c) MCR
(d) Recmind & Selection-Inference
(e) ToT
(f) GDP-Zero & RAP
(g) GoT
Goal
Sub-goal
Valuable Sub-goal
Goal Achievement
Backtracking
Refinement
Majority Vote
Figure 2: Illustration of approaches for valuable sub-goal selection. (a) CoT selects all sub-goals in one sub-goal
sequence. (b) Self-consistency, a variant of CoT, selects sub-goals based on majority votes. (c) MCR, a variant
of CoT, selects sub-goals from multiple sub-goal sequences. (d) Recmind and Selection-Inference, variants of
CoT, select one sub-goal from candidates at each step. (e) ToT explores sub-goals from a tree structure space. (f)
GDP-Zero and RAP, variants of ToT, introduce backpropagation to ToT to balance exploration and exploitation. (g)
GoT models all sub-goals in a graph structure space.
their intention. Then, the goal combined with all
previous sub-goals, selected actions, and sub-goal
results serves as another input to let LLMs extract
useful information from them. Finally, only being
provided with useful information and summarized
intention, LLMs generate the sub-goal result again.
This result will be compared with the original re-
sult. If the regeneration result supports the original
sub-goal result, the original result is considered
correct, and vice versa. In Reflexion (Shinn et al.,
2023) and REFINER (Paul et al., 2024), the authors
try using another instantiation of an LLM to obtain
feedback for the sub-goal result.
Feedback from external tools.
In contrast to
self-evaluation, we could seek the help of external
evaluators to provide feedback.
Some methods (Chen et al., 2023; Wang et al.,
2023a; Rana et al., 2023; Raman et al., 2022) lever-
age the error messages that are pre-built into either
the programming languages or the program of vir-
tual environments to evaluate sub-goal results. Self-
debug (Chen et al., 2023), MAF (Nathani et al.,
2023) and INTERVENOR (Wang et al., 2023a)
leverages unit tests and executors to gain feedback
on generated code snippets. The authors also em-
pirically show that leveraging unit test error mes-
sages leads to superior performance than LLM’s
self-reflection. This finding indicates that exter-
nal evaluators may be more beneficial than self-
reflection thanks to the more precise feedback (Gou
et al., 2023). SayPlan (Rana et al., 2023) and Re-
prompting (Raman et al., 2022) directly leverage
the signs of success or failure with error messages
from the environment as feedback to revise their
actions. GITM (Zhu et al., 2023) designs a struc-
tured feedback template to get detailed knowledge
of the current situation and all of the information
can be obtained from environment API returns.
However, sometimes it is hard to define error
messages in advance. To further address it, LLM-
Planner (Song et al., 2023) explores the possibility
of using Vision-Language Models (VLMs) to pro-
vide feedback by describing the environment. Inner
Monologue (Huang et al., 2022c) and DEPS (Wang
et al., 2023f) are combinations of SayPlan and
LLM-Planner, where VLMs additionally perform
visual question answering to enable a finer-grained
human-computer interaction for valuable informa-
tion. Reflexion (Shinn et al., 2023) proposes vari-
ous heuristic functions to score different tasks (e.g.
exact match for reasoning). CRITIC (Gou et al.,
2023) and Verify-and-Edit (Zhao et al., 2023) em-
ploy search engines to obtain true information as
feedback from the web.
3.5
Further Selection of Valuable Sub-goals
The generated sub-goals in stage (1) may be wrong
or irrelevant to overall goal achievement. To ad-
dress this problem, we could ask LLMs to explore
several sub-goals in each step or even several sub-
goal sequences and then only select those valuable
candidates. In this section, we introduce methods
for such sub-goal post-processing, which can be di-
vided into three stages of development, CoT and its
variants, Tree of thoughts (ToT) (Yao et al., 2023b)
and its variants, and finally Graph of thoughts
(GoT) (Besta et al., 2023). An illustration of these
approaches is shown in Fig. 2 to better distinguish
these methods.
Chain-of-thought and its variants.
We consider
CoT as simply selecting all sub-goals since LLMs
have access to all of the sub-goals during the gen-
eration process, which makes it prone to irrele-
vant sub-goals or wrong sub-goal results. Four
works are proposed to improve the robustness of
CoT. Self-consistency (Wang et al., 2022) prompts
LLMs several times via CoT and then conducts ma-
jority votes based on the similarity of different out-
puts from each COT sequence to decide valuable
sub-goal sequences. The result from the chosen se-
quence is considered the final result. MCR (Yoran
et al., 2023) improves Self-consistency by select-
ing and combining sub-goals from different se-
quences. After getting several sub-goal sequences,
it uses LLMs to extract valuable sub-goals to form
a new sub-goal sequence, dubbed meta-reasoning
path, and predict the result based on the meta
path. Recmind (Wang et al., 2023e) and Selection-
Inference (Creswell et al., 2022) allow multiple
sub-goal candidates at each decomposition step
and choose the most valuable one based on LLM
self-evaluation.
Tree of thoughts and its variants.
ToT (Yao
et al., 2023b) advances over CoT by enabling the
exploration and comparison of sub-goals based on
a tree structure. To make global choices, at each
sub-goal when traversing the tree, a decision can be
made that involves backtracking or looking ahead
based on the implementation of traditional search-
ing algorithms like breadth-first search. For the
comparison of sub-goals, ToT solely relies on the
self-evaluation of LLMs. CoT and its variants can
be considered special cases of ToT, where each
node only has one child node.
RAP (Hao et al., 2023) and GDP-Zero (Yu
et al., 2023a) improve from ToT’s heuristic-based
search to Monte Carlo Tree Search (MCTS). Com-
pared with ToT, MCTS has an additional back-
propagation step, where the score value that indi-
cates the goal achievement is back-propagated to
update the value score of all sub-goals in the sub-
goal sequence. This operation strikes a proper bal-
ance between exploration and exploitation to find
valuable sub-goals efficiently, which makes MCTS
outperform heuristic-based search algorithms on
complex or less structured tasks.
Graph of thoughts.
GoT (Besta et al., 2023) ad-
vances ToT by modeling all of the LLM-generated
sub-goals as a graph. While in ToT, only sub-goals
in a sequence or at the same step can interact with
each other through comparison, lookahead, or back-
tracking, GoT allows the synergy of arbitrary sub-
goals. What’s more, GoT supports sub-goal ag-
gregation to generate a new sub-goal and sub-goal
refinement based on other dependent sub-goals. Fi-
nally, valuable sub-goals are selected via LLM self-
evaluation on the constructed graph.
4
Discussion
Application.
After understanding how each
method leverages goal-oriented prompt engineer-
ing to achieve goals with LLMs, we find it insight-
ful to provide a bird-eye view of the surveyed pa-
per. In Table 10, we classify these papers by their
targeted tasks and the stages involved in our goal-
oriented framework, to help readers better under-
stand the current progress2.
The 50 representative works in our survey cover
11 tasks, including arithmetic reasoning (18 works),
commonsense reasoning (5 works), symbolic rea-
soning (3 works), logical reasoning (5 works), plan-
ning in virtual/real environment (12 works), mul-
tihop question answering (8 works), open-domain
question answering (1 work), code generation (4
works), dialogue systems (6 works), and recom-
mender systems (3 works). We could see that 46%
of the works focus on reasoning, 24% of the works
are for planning and the remaining 30% of the
works are for other tasks.
For reasoning tasks, all the surveyed works in-
volve only a single stage, with goal decomposition,
action execution, sub-goal result evaluation, and
valuable sub-goal selection holding 27% equally.
There is potential for exploring the synergy of
stages, which we will discuss in Section 5. For
planning tasks, goal decomposition and action se-
lection share 27%, and sub-goal result evaluation
shares the rest 46%. No surveyed work for plan-
ning tasks involves the action execution stage and
valuable sub-goal selection stage. This is because
the currently available evaluation benchmarks are
based on either simple real environments or care-
fully designed virtual environments. Actions are
either easy or can be assured to be perfectly exe-
cuted. Nonetheless, as the field develops, we can
expect more complicated environments where there
should be attention paid to action execution and
valuable sub-goal selection stage.
For those minority tasks like code generation and
recommendation, most of the stages remain unex-
plored. With the goal-oriented framework proven
effective in reasoning and planning tasks, it should
2Note that here we only introduce representative
tasks.
For more comprehensive information, we main-
tain
a
list
at
https://github.com/Alex-HaochenLi/
Goal-oriented-Prompt-Engineering.
be valuable to explore the potential of the frame-
work on other tasks and we will discuss it in Sec 5.
Performance.
Furthermore, We compare the per-
formance of the surveyed methods in Appendix B
(Table 2, 3, 4, 5, 6, 7, 8, 9), and there are some note-
worthy findings. In arithmetic reasoning, compared
with standard I-O prompting, on average goal de-
composition (CoT) improves by 22.6%, sub-goal
evaluation (Self-refine (Madaan et al., 2023)) im-
proves by 21.1%, and valuable sub-goal selection
(Self-consistency (Wang et al., 2022)) improves by
32.5%. While valuable sub-goal selection improves
the most in arithmetic reasoning, the improvement
brought by goal decomposition outperforms sub-
goal selection in symbolic reasoning, which indi-
cates that the variations of improvement have rela-
tions to task characteristics. Besides, we observe
a gradual increase in performance when integrat-
ing additional stages into the method. PoT (Chen
et al., 2022) can be considered as a combination of
CoT and action execution, it further improves CoT
by 14.7% in arithmetic reasoning. Similarly, Say-
plan (Rana et al., 2023) in planning tasks, equipped
with goal decomposition, action selection, and sub-
goal evaluation, outperforms solely sub-goal evalu-
ation (LLM-Planner (Song et al., 2023)) by 73.3%
in excitability. Thus, we could expect further im-
provement from stage synergy.
During data collection, we find it hard to com-
pare between methods due to the variety of prompt
templates, the number of in-context examples in-
cluded, and the decoding strategies, even if the
methods are applied to the same LLM in the same
zero-/few-shot setting. With our survey, we hope
to promote a unified evaluation procedure for each
task for fair comparison of future methods.
5
Challenges & Opportunities
In this section, we discuss the challenges and oppor-
tunities of goal-oriented prompt engineering based
on our findings.
Hierarchical Goal Decomposition.
Existing
works only decompose the goal into one layer of
sub-goals, ignoring the fact that the sub-goal itself
can still be complex and multifaceted. To make
sure each sub-goal is simple enough to be handled
by LLMs, researchers are now exploring hierar-
chical decomposition, which further decomposes
sub-goals into simpler "sub-sub-goals". (Li et al.,
2023) is a pioneering work that constructs a dataset
for the script generation task focusing on hierar-
chical decomposition. GITM (Zhu et al., 2023)
employs the idea of hierarchical decomposition by
utilizing a pre-defined sub-goal tree to connect a
sub-goal with even simpler ones. Though a signifi-
cant success rate increase by employing hierarchi-
cal decomposition is reported, a pre-defined tree is
not always accessible. Besides, relying solely on
one tree may not be able to handle more complex
problems, which leaves room for the introduction
of graphs for further improvement.
Efficiency.
The current design of goal-oriented
prompt engineering focuses on improving the accu-
racy of models. Nevertheless, efficiency also plays
a crucial role in the real-world application of the
methods. Researchers should consider the trade-off
between accuracy and efficiency when necessary.
We propose to consider two perspectives of effi-
ciency measurement, task-agnostic efficiency and
task-specific efficiency. Task-agnostic efficiency
aims at measuring the efficiency of model targets
for any task, for instance, the total number of
prompting times before reaching the sub-goal or
final goal. On the other hand, task-specific effi-
ciency is designed to measure efficiency that ap-
pears only in certain tasks. As an example, the
number of iterations that a certain method prompts
LLMs would affect the speed of plan generation
in planning tasks. Such task-agnostic efficiency
is important in scenarios like autonomous driving
path planning when we need a quick response for
better user experiences. On the other hand, the
output plan needs to fulfill other criteria, such as
minimizing traveling time or distance. Here, the
time and distance are task-specific efficiency.
Stage Synergy.
Overall there is a lack of system-
atic synergy of stages in any task. We found two
works that combine their methods, on one stage,
with a method, on another stage, and show perfor-
mance improvement (Rana et al., 2023; Chen et al.,
2022). This suggests the power of synergizing the
stages in the goal-oriented framework and careful
integration of stages would have the potential for
further improvement.
We take the arithmetic reasoning task as an ex-
ample. To solve a math problem, a naive solution
would be to follow Plan-and-solve (Wang et al.,
2023d) to decompose the math problem into a few
logical steps. Then, following MWP (Zhang et al.,
2023), an operation prediction module can be ap-
plied to determine the correct mathematical opera-
tions. LLMs can select the operands for performing
the operation. Then, LLMs could call an external
calculator to calculate the equation to ensure an ac-
curate result. With the result, we could follow Re-
flexion (Shinn et al., 2023) to ask LLMs to double-
check the intermediate steps again. We could fur-
ther repeat the described process to get multiple se-
quences of sub-goals and follow MCR (Yoran et al.,
2023) to remove less valuable equations for more
accurate answers. Of course, a naive combination
of methods from all stages is heavy, requiring ad-
ditional attention paid to efficiency. Additionally,
it is also important to consider the between-stages
interaction. For the same math problem, if we ap-
ply a calculator for calculation (action execution)
to guarantee a correct answer, we should be careful
when designing the result evaluation stage. If we
use LLMs for result evaluation, the correct result
from the calculator may inversely be considered as
wrong due to the hallucination problem.
Broader Applications.
From our summarization,
we observe an absence of study in certain stages
for some tasks, such as code generation and rec-
ommender system. There is potential to improve
current methods by studying the unexplored stages.
Moreover, goal-oriented prompt engineering has
the potential for broader applications beyond the
11 tasks summarized in this paper.
In code generation, all of the surveyed meth-
ods focus on leveraging sub-goal evaluation to re-
fine the code generation result. Additional refine-
ment may be done by incorporating other stages.
Given a description of the desired code snippets, we
could first decompose it into several required sub-
functions, and then select appropriate algorithms to
implement them. As for the implementation stage
of these sub-functions, we could search relevant
documents of potential well-written function APIs
for reference instead of generating the code fully
based on LLMs’ knowledge. APIs in external li-
braries are regularly updated which can perform
better than the static knowledge stored in LLMs.
In recommender systems, works have been pro-
posed for enhancing action execution and sub-goal
result evaluation. There is an absence of work on
action selection. Take keeping user retention as the
overall goal, one sub-goal might be to make sure
that users are recommended with their preferred
items. There are various actions we could take to
predict the preference of a user for a certain item,
such as inferring based on the click rate of similar
items and inferring based on the watching time of
relevant advertisements. Careful design on dynam-
ically selecting between available actions based
on the characteristics of the user may enable more
accurate prediction.
For tasks beyond the scope of this paper, here
we take code review as an example. Code review
is a crucial but cumbersome task in software devel-
opment. It ensures the quality and maintainability
of code but requires a reviewer to spend a lot of
time doing the systematic examination. Automatic
code review can help save such significant effort.
Due to the coding ability shown by recent LLMs,
there is potential to surpass existing static analysis
tools in automatic code review if equipped with our
goal-oriented framework. We found one primary
study (Tufano et al., 2024), which focuses on action
selection, already shows that reducing the action
space by identifying the type of request changes
can significantly improve the success rate of Chat-
GPT. Here we give another possibility which cov-
ers more stages of our framework. With reviewing
code as the goal, sub-goals can be documentation
quality, function reusability, code style consistency,
variable naming readability, and so on. Taking
variable naming readability as the sub-goal, we
can have LLMs select action between checking the
self-consistency of the variable names or compar-
ing those names with the pre-defined naming con-
vention (the action selection stage). Subsequently,
LLMs can execute the action by either calling an ex-
ternal tool for similarity measurement or following
given heuristic rules (the action execution stage).
After going through all the sub-goals (i.e. review-
ing criteria), LLMs can revise the given code based
on reviews from each perspective.
6
Conclusion
In this paper, we provide a review of existing re-
search on LLMs prompting from a goal-oriented
perspective. We explicitly summarize the task-
agnostic internal steps involved in designing and
refining prompting procedures into five stages,
classify the surveyed methods by their targeted
tasks, and provide a comprehensive analysis on
task coverage and performance. Though signifi-
cant progress has been made in this field, further
research is needed to fully unleash the potential
by studying hierarchical goal decomposition, the
synergy of stages, application to other tasks, and
considering efficiency problems. We hope this pa-
per can serve as an overview of the current state
of goal-oriented prompt engineering and stimulate
further research on this important topic.
References
Gavin Abercrombie, Amanda Curry, Tanvi Dinkar, Ver-
ena Rieser, and Zeerak Talat. 2023. Mirages. on
anthropomorphism in dialogue systems. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing, pages 4776–4790,
Singapore. Association for Computational Linguis-
tics.
Xavier Amatriain. 2024. Prompt design and engineer-
ing: Introduction and advanced methods.
arXiv
preprint arXiv:2401.14423.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732.
James T Austin and Jeffrey B Vancouver. 1996. Goal
constructs in psychology: Structure, process, and
content. Psychological bulletin, 120(3):338.
Maciej Besta, Nils Blach, Ales Kubicek, Robert Ger-
stenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz
Lehmann, Michal Podstawski, Hubert Niewiadomski,
Piotr Nyczyk, et al. 2023. Graph of thoughts: Solv-
ing elaborate problems with large language models.
arXiv preprint arXiv:2308.09687.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde De Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021.
Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022.
Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588.
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2023. Teaching large language models
to self-debug. arXiv preprint arXiv:2304.05128.
Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang
Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,
Bing Qin, and Ting Liu. 2023. A survey of chain of
thought reasoning: Advances, frontiers and future.
arXiv preprint arXiv:2309.15402.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168.
Antonia Creswell, Murray Shanahan, and Irina Higgins.
2022. Selection-inference: Exploiting large language
models for interpretable logical reasoning. In The
Eleventh International Conference on Learning Rep-
resentations.
Yang Deng, Wenqiang Lei, Lizi Liao, and Tat-Seng
Chua. 2023. Prompting and evaluating large lan-
guage models for proactive dialogues: Clarification,
target-guided, and non-collaboration. arXiv preprint
arXiv:2305.13626.
Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt
Gardner. 2022. Successive prompting for decom-
posing complex questions. In Proceedings of the
2022 Conference on Empirical Methods in Natural
Language Processing, pages 1251–1265.
Kevin A Fischer. 2023. Reflective linguistic program-
ming (rlp): A stepping stone in socially-aware agi
(socialagi). arXiv preprint arXiv:2305.12647.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2023. PAL: Program-aided language
models. In Proceedings of the 40th International
Conference on Machine Learning, volume 202 of
Proceedings of Machine Learning Research, pages
10764–10799. PMLR.
Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge,
and Yongfeng Zhang. 2022. Recommendation as
language processing (rlp): A unified pretrain, person-
alized prompt & predict paradigm (p5). In Proceed-
ings of the 16th ACM Conference on Recommender
Systems, pages 299–315.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics, 9:346–
361.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong
Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
2023. Critic: Large language models can self-correct
with tool-interactive critiquing.
arXiv preprint
arXiv:2305.11738.
Maitrey Gramopadhye and Daniel Szafir. 2023. Gener-
ating executable action plans with environmentally-
aware language models. In 2023 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems
(IROS), pages 3568–3575. IEEE.
Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhent-
ing Qi, Martin Riddell, Wenfei Zhou, James Coady,
David Peng, Yujie Qiao, Luke Benson, et al. 2022.
Folio: Natural language reasoning with first-order
logic. arXiv preprint arXiv:2209.00840.
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong,
Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023.
Reasoning with language model is planning with
world model. arXiv preprint arXiv:2305.14992.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020. Constructing a multi-hop
qa dataset for comprehensive evaluation of reasoning
steps. arXiv preprint arXiv:2011.01060.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren
Etzioni, and Nate Kushman. 2014. Learning to solve
arithmetic word problems with verb categorization.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 523–533.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin
Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.
2022a.
Large language models can self-improve.
arXiv preprint arXiv:2210.11610.
Jie Huang and Kevin Chen-Chuan Chang. 2022. To-
wards reasoning in large language models: A survey.
arXiv preprint arXiv:2212.10403.
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, et al.
2023a. A survey on hallucination in large language
models: Principles, taxonomy, challenges, and open
questions. arXiv preprint arXiv:2311.05232.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Igor Mordatch. 2022b. Language models as zero-
shot planners: Extracting actionable knowledge for
embodied agents. In International Conference on
Machine Learning, pages 9118–9147. PMLR.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Liang, Pete Florence, Andy Zeng, Jonathan Tompson,
Igor Mordatch, Yevgen Chebotar, et al. 2022c. Inner
monologue: Embodied reasoning through planning
with language models. In 6th Annual Conference on
Robot Learning.
Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu
Lian, and Xing Xie. 2023b. Recommender ai agent:
Integrating large language models for interactive rec-
ommendations. arXiv preprint arXiv:2308.16505.
Shima Imani, Liang Du, and Harsh Shrivastava. 2023.
MathPrompter: Mathematical reasoning using large
language models. In Proceedings of the 61st An-
nual Meeting of the Association for Computational
Linguistics (Volume 5: Industry Track), pages 37–
42, Toronto, Canada. Association for Computational
Linguistics.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of halluci-
nation in natural language generation. ACM Comput-
ing Surveys, 55(12):1–38.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao
Fu, Kyle Richardson, Peter Clark, and Ashish Sab-
harwal. 2022. Decomposed prompting: A modular
approach for solving complex tasks. In The Eleventh
International Conference on Learning Representa-
tions.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. Advances in
neural information processing systems, 35:22199–
22213.
Soochan Lee and Gunhee Kim. 2023. Recursion of
thought: A divide-and-conquer approach to multi-
context reasoning with language models.
arXiv
preprint arXiv:2306.06891.
Xinze Li, Yixin Cao, Muhao Chen, and Aixin Sun. 2023.
Take a break in the middle: Investigating subgoals
towards hierarchical script generation. arXiv preprint
arXiv:2305.10907.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. arXiv preprint arXiv:1705.04146.
Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,
Shiqi Zhang, Joydeep Biswas, and Peter Stone.
2023a. Llm+ p: Empowering large language models
with optimal planning proficiency. arXiv preprint
arXiv:2304.11477.
Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang
Zhou, and Yan Zhang. 2023b. Is chatgpt a good
recommender? a preliminary study. arXiv preprint
arXiv:2304.10149.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023c. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Computing Surveys, 55(9):1–35.
Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan,
Guoliang Dong, Peng Di, Wenhai Wang, and
Dongxia Wang. 2023d. Prompting frameworks for
large language models: A survey. arXiv preprint
arXiv:2311.12785.
Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and
Kai-Wei Chang. 2023. A survey of deep learning for
mathematical reasoning. In Proceedings of the 61st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 14605–
14631, Toronto, Canada. Association for Computa-
tional Linguistics.
Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,
Delip Rao, Eric Wong, Marianna Apidianaki, and
Chris Callison-Burch. 2023.
Faithful chain-of-
thought reasoning. arXiv preprint arXiv:2301.13379.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
et al. 2023. Self-refine: Iterative refinement with
self-feedback. arXiv preprint arXiv:2303.17651.
Ning Miao, Yee Whye Teh, and Tom Rainforth.
2023.
Selfcheck: Using llms to zero-shot check
their own step-by-step reasoning.
arXiv preprint
arXiv:2308.00436.
Deepak Nathani, David Wang, Liangming Pan, and
William Wang. 2023. MAF: Multi-aspect feedback
for improving reasoning in large language models.
In Proceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing, pages
6591–6616, Singapore. Association for Computa-
tional Linguistics.
Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Jus-
tifying recommendations using distantly-labeled re-
views and fine-grained aspects. In Proceedings of
the 2019 conference on empirical methods in natural
language processing and the 9th international joint
conference on natural language processing (EMNLP-
IJCNLP), pages 188–197.
Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang,
Armando Solar-Lezama, Joshua Tenenbaum, and
Roger Levy. 2023. LINC: A neurosymbolic approach
for logical reasoning by combining language models
with first-order logic provers. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing, pages 5153–5176, Singapore.
Association for Computational Linguistics.
Liangming Pan, Alon Albalak, Xinyi Wang, and
William Wang. 2023. Logic-LM: Empowering large
language models with symbolic solvers for faithful
logical reasoning. In Findings of the Association
for Computational Linguistics: EMNLP 2023, pages
3806–3824, Singapore. Association for Computa-
tional Linguistics.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021.
Are nlp models really able to solve
simple math word problems?
arXiv preprint
arXiv:2103.07191.
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-
riz Borges, Antoine Bosselut, Robert West, and Boi
Faltings. 2024. REFINER: Reasoning feedback on
intermediate representations. In Proceedings of the
18th Conference of the European Chapter of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1100–1126, St. Julian’s, Malta.
Association for Computational Linguistics.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah A Smith, and Mike Lewis. 2022a. Measuring
and narrowing the compositionality gap in language
models. arXiv preprint arXiv:2210.03350.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah A Smith, and Mike Lewis. 2022b. Measuring
and narrowing the compositionality gap in language
models. arXiv preprint arXiv:2210.03350.
Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li,
Tingwu Wang, Sanja Fidler, and Antonio Torralba.
2018. Virtualhome: Simulating household activities
via programs. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
8494–8502.
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,
Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,
and Huajun Chen. 2022.
Reasoning with lan-
guage model prompting: A survey. arXiv preprint
arXiv:2212.09597.
Shreyas Sundara Raman, Vanya Cohen, Eric Rosen,
Ifrah Idrees, David Paulius, and Stefanie Tellex. 2022.
Planning with large language models via corrective
re-prompting. In NeurIPS 2022 Foundation Models
for Decision Making Workshop.
Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-
Chakra, Ian Reid, and Niko Suenderhauf. 2023.
Sayplan: Grounding large language models using
3d scene graphs for scalable task planning. arXiv
preprint arXiv:2307.06135.
Subhro Roy and Dan Roth. 2016.
Solving gen-
eral arithmetic word problems.
arXiv preprint
arXiv:1608.01413.
Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha,
Vinija Jain, Samrat Mondal, and Aman Chadha.
2024. A systematic survey of prompt engineering in
large language models: Techniques and applications.
arXiv preprint arXiv:2402.07927.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
arXiv preprint arXiv:2302.04761.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
huggingface. arXiv preprint arXiv:2303.17580.
Noah Shinn, Beck Labash, and Ashwin Gopinath.
2023.
Reflexion: an autonomous agent with dy-
namic memory and self-reflection. arXiv preprint
arXiv:2303.11366.
Chan Hee Song, Jiaman Wu, Clayton Washington,
Brian M Sadler, Wei-Lun Chao, and Yu Su. 2023.
Llm-planner: Few-shot grounded planning for em-
bodied agents with large language models. In Pro-
ceedings of the IEEE/CVF International Conference
on Computer Vision, pages 2998–3009.
Simeng Sun, Yang Liu, Shuohang Wang, Chenguang
Zhu, and Mohit Iyyer. 2023. Pearl: Prompting large
language models to plan and execute actions over
long documents. arXiv preprint arXiv:2305.14564.
Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter
Clark. 2020. Proofwriter: Generating implications,
proofs, and abductive statements over natural lan-
guage. arXiv preprint arXiv:2012.13048.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2018. Commonsenseqa: A question
answering challenge targeting commonsense knowl-
edge. arXiv preprint arXiv:1811.00937.
Rosalia Tufano, Ozren Dabi´c, Antonio Mastropaolo,
Matteo Ciniselli, and Gabriele Bavota. 2024. Code
review automation: Strengths and weaknesses of the
state of the art. IEEE Transactions on Software Engi-
neering.
Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui,
Ning Ding, Zhiyuan Liu, and Ge Yu. 2023a. Inter-
venor: Prompt the coding ability of large language
models with the interactive chain of repairing. arXiv
preprint arXiv:2311.09868.
Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei
Mi, Weichao Wang, Yasheng Wang, Wai-Chung
Kwan, Irwin King, and Kam-Fai Wong. 2023b.
Large language models as source planner for person-
alized knowledge-grounded dialogue. arXiv preprint
arXiv:2310.08840.
Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong
Wang, Bin Liang, Ruifeng Xu, and Kam-Fai Wong.
2023c. Cue-cot: Chain-of-thought prompting for
responding to in-depth dialogue questions with llms.
arXiv preprint arXiv:2305.11792.
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao
Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,
Xu Chen, Yankai Lin, et al. 2024. A survey on large
language model based autonomous agents. Frontiers
of Computer Science, 18(6):186345.
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,
Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
2023d. Plan-and-solve prompting: Improving zero-
shot chain-of-thought reasoning by large language
models. arXiv preprint arXiv:2305.04091.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,
Ed H Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. 2022. Self-consistency improves
chain of thought reasoning in language models. In
The Eleventh International Conference on Learning
Representations.
Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang,
Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang
Huang, Yanbin Lu, and Yingzhen Yang. 2023e. Rec-
mind: Large language model powered agent for rec-
ommendation. arXiv preprint arXiv:2308.14296.
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and
Yitao Liang. 2023f. Describe, explain, plan and se-
lect: Interactive planning with large language models
enables open-world multi-task agents. arXiv preprint
arXiv:2302.01560.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in Neural
Information Processing Systems, 35:24824–24837.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W Cohen, Ruslan Salakhutdinov, and
Christopher D Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. arXiv preprint arXiv:1809.09600.
Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, and Erik
Cambria. 2023. Logical reasoning over natural lan-
guage as knowledge representation: A survey. arXiv
preprint arXiv:2303.12023.
Jing Yao, Wei Xu, Jianxun Lian, Xiting Wang, Xiaoyuan
Yi, and Xing Xie. 2023a. Knowledge plugins: En-
hancing large language models for domain-specific
recommendations. arXiv preprint arXiv:2311.10779.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L Griffiths,
Yuan Cao,
and Karthik
Narasimhan. 2023b. Tree of thoughts: Deliberate
problem solving with large language models. arXiv
preprint arXiv:2305.10601.
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel
Deutch, and Jonathan Berant. 2023.
Answering
questions by meta-reasoning over multiple chains
of thought. arXiv preprint arXiv:2304.13007.
Xiao Yu, Maximillian Chen, and Zhou Yu. 2023a.
Prompt-based monte-carlo tree search for goal-
oriented dialogue policy planning. arXiv preprint
arXiv:2305.13660.
Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jia-
jun Chen. 2023b. Towards better chain-of-thought
prompting strategies: A survey.
arXiv preprint
arXiv:2310.04959.
Mengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi
Feng, and Andrew Lan. 2023. Interpretable math
word problem solution generation via step-by-step
planning. arXiv preprint arXiv:2306.00784.
Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei
Qin, and Lidong Bing. 2023. Verify-and-edit: A
knowledge-enhanced chain-of-thought framework.
In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 5823–5840, Toronto, Canada.
Association for Computational Linguistics.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In In-
ternational Conference on Machine Learning, pages
12697–12706. PMLR.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Claire Cui, Olivier Bousquet, Quoc V Le, et al. 2022.
Least-to-most prompting enables complex reasoning
in large language models. In The Eleventh Interna-
tional Conference on Learning Representations.
Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-
jie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu,
Xiaogang Wang, et al. 2023. Ghost in the minecraft:
Generally capable agents for open-world enviroments
via large language models with text-based knowledge
and memory. arXiv preprint arXiv:2305.17144.
A
Related Surveys
While several surveys have explored LLM prompt-
ing, none have examined it from a goal-oriented
perspective or explicitly summarized the task-
agnostic internal steps involved in designing and
refining prompting procedures. Some surveys sum-
marize studies that focus on specific tasks or prob-
lems. (Qiao et al., 2022; Huang and Chang, 2022;
Lu et al., 2023; Yang et al., 2023) focus solely on
reasoning tasks and (Wang et al., 2024) on planning
tasks. (Huang et al., 2023a) deals with the halluci-
nation problem of LLMs. Other studies focus on
generally applicable prompting but different sub-
fields. (Amatriain, 2024; Sahoo et al., 2024) simply
listed out recent techniques without systematic cat-
egorization. (Yu et al., 2023b; Chu et al., 2023)
study CoT and its XoT variants. (Liu et al., 2023d)
focus on the prompting framework for facilitating
the interaction between LLMs with external worlds.
(Liu et al., 2023c) focuses on methods for learn-
able continuous prompt, while our work studies
discrete prompt. As many of the will-functioned
LLMs are not open-sourced (e.g. GPT-3.5, GPT4),
discrete prompts became the focus of recent stud-
ies on LLM prompting and therefore we choose to
focus on discrete prompting.
B
Performance Comparison
In this section, we compare the performance of sur-
veyed methods under several tasks, including arith-
metic reasoning (Table 2), commonsense reasoning
(Table 3), symbolic reasoning (Table 4), logical
reasoning (Table 5), planning (Table 6), multi-hop
question answering (Table 7), code generation (Ta-
ble 8), and recommender systems (Table 9). The
empirical results are collected from correspond-
ing papers or other papers where they serve as
baselines. Here, the benchmarks we chose are the
ones on which the majority of methods are evalu-
ated. For a full list of benchmarks, we maintain
it
at
https://github.com/Alex-HaochenLi/
Goal-oriented-Prompt-Engineering.
Note
that due to variations in experimental settings like
prompt template, decoding strategy, and numbers
of in-context examples, even the methods that share
the same zero-/few-shot setting and LLM may not
be fairly comparable. Therefore, these tables only
provide a rough trend of performance, and we don’t
make conclusions stating which method works best
in certain contexts. We mainly aim to show that the
introduction of goal-oriented prompt design can
consistently bring improvement.
Methods
Stage
Setting
Model
GSM8K
SVAMP
AQuA
AddSub
MultiArith
(Cobbe et al., 2021)
(Patel et al., 2021)
(Ling et al., 2017)
(Hosseini et al., 2014)
(Roy and Roth, 2016)
Zero-shot CoT (Kojima et al., 2022)
①
zero-shot
text-davinci-003
56.4
69.9
38.9
85.3
83.8
Few-shot CoT (Wei et al., 2022)
①
few-shot
text-davinci-003
58.4
80.3
48.4
91.6
93.6
Plan-and-solve (Wang et al., 2023d)
①
zero-shot
text-davinci-003
59.3
75.7
46.0
92.2
91.8
Least-to-most (Zhou et al., 2022)
①
few-shot
code-davinci-002
38.3
80.3
40.6
-
74.0
PoT (Chen et al., 2022)
③
zero-shot
text-davinci-003
57.0
70.8
43.9
85.1
92.2
Toolformer (Schick et al., 2023)
③
zero-shot
GPT-J
-
29.4
-
-
-
PoT (Chen et al., 2022)
③
few-shot
code-davinci-002
71.6
85.2
54.1
-
-
PAL (Gao et al., 2023)
③
few-shot
code-davinci-002
72.0
79.4
-
92.5
99.2
Faithful CoT (Lyu et al., 2023)
③
few-shot
code-davinci-002
72.3
83.4
47.2
-
98.8
RAP (Hao et al., 2023)
④
few-shot
LLaMA-33B
40.0
-
-
-
-
Self-refine (Madaan et al., 2023)
④
few-shot
text-davinci-003
64.1
67.6
-
-
-
Self-consistency (Wang et al., 2022)
⑤
few-shot
code-davinci-002
78.0
86.8
52.0
91.6
100.0
Table 2: Performance comparison on arithmetic reasoning. The evaluation metric is solve rate (%).
Metric
Methods
Stage
Setting
Model
CSQA
StrategyQA
(Talmor et al., 2018)
(Geva et al., 2021)
Solve rate
Zero-shot CoT (Kojima et al., 2022)
①
zero-shot
text-davinci-003
65.2
63.8
Plan-and-solve (Wang et al., 2023d)
①
zero-shot
text-davinci-003
71.9
65.4
Few-shot CoT (Wei et al., 2022)
①
few-shot
text-davinci-003
78.3
71.2
Few-shot CoT (Wei et al., 2022)
①
few-shot
code-davinci-002
79.0
73.4
Self-consistency (Wang et al., 2022)
⑤
few-shot
code-davinci-002
81.5
79.8
F1
CoT (Wei et al., 2022)
①
few-shot
code-davinci-002
-
70.0
Self-ask (Press et al., 2022a)
①
few-shot
code-davinci-002
-
69.3
MCR (Yoran et al., 2023)
⑤
few-shot
code-davinci-002
-
73.6
Self-consistency (Wang et al., 2022)
⑤
few-shot
code-davinci-002
-
72.2
Table 3: Performance comparison on commonsense reasoning under solve rate (%) and F1 (%). F1 is computed by
treating prediction and ground truth answers as bags of tokens and computing their precision and recall.
Methods
Stage
Setting
Model
last letter concatenation
coinflip
(Wei et al., 2022)
(Wei et al., 2022)
I-O Prompting
-
few-shot
text-davinci-002
5.8
49.0
Zero-shot CoT (Kojima et al., 2022)
①
zero-shot
text-davinci-003
64.8
96.8
Plan-and-solve (Wang et al., 2023d)
①
zero-shot
text-davinci-003
75.2
99.6
Few-shot CoT (Wei et al., 2022)
①
few-shot
text-davinci-003
70.6
100.0
Few-shot CoT (Wei et al., 2022)
①
few-shot
code-davinci-002
70.4
99.0
Least-to-most (Zhou et al., 2022)
①
few-shot
code-davinci-002
94.0
-
Self-consistency (Wang et al., 2022)
⑤
few-shot
code-davinci-002
73.4
99.5
Table 4: Performance comparison on symbolic reasoning. The evaluation metric is solve rate (%).
Methods
Stage
Setting
Model
FOLIO
ProofWriter
(Han et al., 2022)
(Tafjord et al., 2020)
I-O Prompting
-
zero-shot
gpt-3.5-turbo
48.4
36.4
CoT (Wei et al., 2022)
①
few-shot
gpt-3.5-turbo
54.9
43.6
Logic-LM (Pan et al., 2023)
③
few-shot
gpt-3.5-turbo
62.7
58.3
LINC (Olausson et al., 2023)
③
few-shot
gpt-3.5-turbo
62.6
96.4
LINC (Olausson et al., 2023)
③
few-shot
GPT-4
72.5
98.3
Table 5: Performance comparison on logical reasoning. The evaluation metric is solve rate (%).
Dataset
Methods
Stage
Setting
Model
Evaluation Metric
Executability
Correctness
Home+Office
LLM+P (Liu et al., 2023a)
①
few-shot
GPT-4
0.0
33.3
LLM-Planner (Song et al., 2023)
④
few-shot
GPT-4
13.3
66.7
(Rana et al., 2023)
SayPlan (Rana et al., 2023)
①②④
few-shot
GPT-4
86.6
73.3
VirtualHome
Zero-shot Planner (Huang et al., 2022b)
②
few-shot
GPT2-large
16.4
33.3
Zero-shot Planner (Huang et al., 2022b)
②
few-shot
Codex
78.4
46.1
(Puig et al., 2018)
SALP (Gramopadhye and Szafir, 2023)
②
few-shot
GPT2-large
50.8
49.0
Re-prompting (Raman et al., 2022)
②
few-shot
Codex
98.9
41.8
Table 6: Performance comparison on planning in virtual environment. The evaluation metrics are executability (%)
and correctness (%). Executability measures if the generated actions can be correctly parsed in the environment,
while Correctness is a human-annotated metric that assesses whether the given steps could accomplish the given
task.
Metric
Methods
Stage
Setting
Model
2WikiMultihopQA
Bamboogle
HotpotQA
(Ho et al., 2020)
(Press et al., 2022b)
(Yang et al., 2018)
Acc.
I-O Prompting
-
few-shot
Codex
25.4
17.6
-
CoT (Wei et al., 2022)
①
few-shot
Codex
29.8
46.4
-
Least-to-most (Zhou et al., 2022)
①
few-shot
Codex
29.0
-
-
Self-ask (Press et al., 2022a)
①
few-shot
Codex
40.1
60.0
-
F1
CoT (Wei et al., 2022)
①
few-shot
Codex
67.2
64.7
56.4
Self-ask (Press et al., 2022a)
①
few-shot
Codex
63.8
64.6
50.2
DecomP (Khot et al., 2022)
①
zero-shot
Codex
64.1
25.4
49.9
MCR (Yoran et al., 2023)
⑤
few-shot
Codex
67.9
66.5
57.0
Self-consistency (Wang et al., 2022)
⑤
few-shot
Codex
65.4
65.0
56.4
Table 7: Performance comparison on multihop question answering under accuracy (%) and F1 (%). F1 is computed
by treating prediction and ground truth answers as bags of tokens and computing their precision and recall.
Methods
Stage
Setting
Model
HumanEval
MBPP
(Chen et al., 2021)
(Austin et al., 2021)
Zero-shot I-O Prompting
-
zero-shot
gpt-3.5-turbo
62.2
41.6
Few-shot I-O Prompting
-
few-shot
gpt-3.5-turbo
65.2
40.6
CoT (Wei et al., 2022)
①
zero-shot
gpt-3.5-turbo
66.5
48.8
Self-refine (Madaan et al., 2023)
④
zero-shot
gpt-3.5-turbo
65.2
48.8
INTERVENOR (Wang et al., 2023a)
④
zero-shot
gpt-3.5-turbo
75.6
69.8
Reflexion (Shinn et al., 2023)
④
zero-shot
GPT-4
91.0
77.1
Self-debug (Chen et al., 2023)
④
few-shot
code-davinci-002
-
75.6
Table 8: Performance comparison on code generation. The evaluation metric is Pass@1 (%).
Methods
Stage
Setting
Model
Amazon Review
Yelp
(Ni et al., 2019)
(Geng et al., 2022)
I-O Prompting (Liu et al., 2023b)
-
zero-shot
gpt-3.5-turbo
1.1897
1.2359
CoT (Wei et al., 2022)
①
zero-shot
gpt-3.5-turbo
0.8612
1.1673
Selection-Inference (Creswell et al., 2022)
⑤
zero-shot
gpt-3.5-turbo
0.7883
1.0009
I-O Prompting (Liu et al., 2023b)
-
few-shot
gpt-3.5-turbo
0.7327
1.0016
CoT (Wei et al., 2022)
①
few-shot
gpt-3.5-turbo
0.7167
0.9794
ToT (Yao et al., 2023b)
⑤
few-shot
gpt-3.5-turbo
0.7059
0.9766
Selection-Inference (Creswell et al., 2022)
⑤
few-shot
gpt-3.5-turbo
0.6892
0.9698
Table 9: Performance comparison on rating prediction in recommender system. The evaluation metric is Mean
Absolute Error. Here I-O prompting means that LLMs are directly prompted for the final prediction.
Application
Sub-category
Stage
Method
Reasoning
Arithmetic
①
CoT (Wei et al., 2022), Least-to-most (Zhou et al., 2022), Plan-and-
solve (Wang et al., 2023d), Successive Prompt (Dua et al., 2022),
RoT (Lee and Kim, 2023)
②
MWP (Zhang et al., 2023)
③
PoT (Chen et al., 2022), Toolformer (Schick et al., 2023), Faithful
CoT (Lyu et al., 2023), PAL (Gao et al., 2023), MathPrompter (Imani
et al., 2023)
④
Self-refine (Madaan et al., 2023), RAP (Hao et al., 2023), Self-
Check (Miao et al., 2023), REFINER (Paul et al., 2024), CRITIC (Gou
et al., 2023), MAF (Nathani et al., 2023)
⑤
Self-consistency (Wang et al., 2022)
Commonsense
①
CoT (Wei et al., 2022), Plan-and-solve (Wang et al., 2023d)
③
Toolformer (Schick et al., 2023)
⑤
Self-consistency (Wang et al., 2022), MCR (Yoran et al., 2023)
Symbolic
①
Plan-and-solve (Wang et al., 2023d)
③
PAL (Gao et al., 2023)
⑤
Self-consistency (Wang et al., 2022)
Logical
②
DecomP (Khot et al., 2022)
③
LINC (Olausson et al., 2023), Logic-LM (Pan et al., 2023)
④
RAP (Hao et al., 2023)
⑤
Selection-Inference (Creswell et al., 2022)
Planning
Virtual Env
①
LLM+P (Liu et al., 2023a), SayPlan (Rana et al., 2023), DEPS (Wang
et al., 2023f), GITM (Zhu et al., 2023)
②
Zero-shot Planner (Huang et al., 2022b), Re-prompting (Raman et al.,
2022), SALP (Gramopadhye and Szafir, 2023), SayPlan (Rana et al.,
2023),
④
Re-prompting (Raman et al., 2022), RAP (Hao et al., 2023), Reflex-
ion (Shinn et al., 2023), SayPlan (Rana et al., 2023), DEPS (Wang et al.,
2023f), GITM (Zhu et al., 2023), Inner Monologue (Huang et al., 2022c),
LLM-Planner (Song et al., 2023)
Real Env
④
Inner Monologue (Huang et al., 2022c)
Question Answering
Multihop QA
①
DecomP (Khot et al., 2022), Self-ask (Press et al., 2022a)
②
PEARL (Sun et al., 2023)
③
Faithful CoT (Lyu et al., 2023)
④
Reflexion (Shinn et al., 2023), CRITIC (Gou et al., 2023), Verify-and-
Edit (Zhao et al., 2023)
⑤
MCR (Yoran et al., 2023)
Open domain QA
③
Toolformer (Schick et al., 2023)
Code Generation
④
Reflexion (Shinn et al., 2023), Self-debug (Chen et al., 2023), Self-
refine (Madaan et al., 2023), INTERVENOR (Wang et al., 2023a)
Dialogue
②
ProCoT (Deng et al., 2023), RLP (Fischer, 2023), Cue-CoT (Wang et al.,
2023c), SAFARI (Wang et al., 2023b)
④
Self-refine (Madaan et al., 2023)
⑤
GDP-Zero (Yu et al., 2023a),
Recommendation
③
Recmind (Wang et al., 2023e), DOKE (Yao et al., 2023a), InteRecA-
gent (Huang et al., 2023b)
⑤
Recmind (Wang et al., 2023e)
Table 10: Applicable tasks from existing works. The stage column indicates the stages involved in the corresponding
methods with regard to the goal-oriented framework in Fig. 1. Note that the methods in the table are not exclusively
applicable to the tasks listed in the table to which they belong. We list them based on the evaluated tasks in their
original papers.
